# --- Imports ---
import os
import json

import pandas as pd

from utils.utils import *
from benchmarking.classes.SpecificLLMBenchmark import *
from benchmarking.classes.BaseLLMBenchmark import BaseLLMBenchmark


# --- Functions ---
def get_benchmark_instance(model: str) -> BaseLLMBenchmark:
    """
    Get a benchmark instance for a specific Language Model (LLM) model.

    Parameters:
        model (str): The name of the LLM model for which the benchmark instance is requested.

    Returns:
        BaseLLMBenchmark: An instance of the benchmark class corresponding to the specified LLM model.

    Raises:
        ValueError: If no benchmark instance is available for the provided LLM model.
    """
    benchmarks = {
        "Mixtral-8x7B": Mixtral8x7BBenchmark,
        "Meta-Llama3-8B": LlamaV38BBenchmark,
        "ChatGPT": ChatGPTBenchmark,
    }
    if model in benchmarks:
        return benchmarks[model]()
    else:
        raise ValueError(f"No benchmark instance available for model: {model}")


def read_input_data(base_dir: str, step_dir: str) -> tuple:
    """
    Get the expected response data, source data, and template directory path for a given base directory and step directory.

    Parameters:
        base_dir (str): The base directory where the input data is located.
        step_dir (str): The directory for a specific step within the base directory.

    Returns:
        tuple: A tuple containing the expected response data, source data, and template directory path.

    Example:
        expected_response_data, source_data, template_dir = read_input_data("base_directory", "step_directory")
    """
    input_dir = os.path.join(base_dir, "input", step_dir)

    expected_response_path = os.path.join(
        input_dir, "expected_response", f"expected_response_{step_dir}.json"
    )
    expected_response_data = load_json_file(expected_response_path)["data"]

    source_path = os.path.join(input_dir, "source", f"source_{step_dir}.json")
    source_data = load_json_file(source_path)["data"]

    template_dir = os.path.join(input_dir, "templates")

    return expected_response_data, source_data, template_dir


def evaluate_responses(
    benchmark: BaseLLMBenchmark,
    expected_response: str,
    source_data: str,
    template_dir: str,
    output_path: str,
) -> list:
    """
    Evaluate the responses generated by a Language Model (LLM) for multiple templates.

    Parameters:
        benchmark (BaseLLMBenchmark): An instance of the BaseLLMBenchmark class for evaluating LLM responses.
        expected_response (str): The expected response for comparison with LLM-generated responses.
        source_data (str): The source data used for creating prompts.
        template_dir (str): The directory containing template files for generating prompts.
        output_path (str): The directory where LLM-generated responses are stored.

    Returns:
        list: A list of dictionaries containing evaluation results for each template's LLM response. Each dictionary includes metrics such as accuracy, coherence, creativity, engagement, and sentiment alignment.

    Notes:
        - This function iterates through each template file in the template directory.
        - For each template, it creates a prompt using the template data and the provided source data.
        - It reads the LLM-generated response from the corresponding output file.
        - The LLM response is evaluated using the BaseLLMBenchmark instance and the expected response.
        - The evaluation results for each template are stored in a list of dictionaries.
        - The function returns the list of evaluation results for all templates processed.
    """
    all_results = []

    for template_file in os.listdir(template_dir):
        template_path = os.path.join(template_dir, template_file)
        template_data = load_json_file(template_path)["data"]
        prompt = create_prompt(template_data, source_data)

        output_filename = template_file.replace("template", "llm_response")
        output_filepath = os.path.join(output_path, output_filename)
        llm_response = read_generated_response(output_filepath)

        results = benchmark.evaluate_response(prompt, expected_response, llm_response)
        all_results.append(results)
        # print(f"Results for {template_file}: {results}")

    return all_results


def save_benchmark_results(
    benchmark: BaseLLMBenchmark,
    metrics_fname: str,
    metrics_path: str,
    figures_path: str,
    results: list,
) -> None:
    """
    Save the benchmarking results to files.

    Parameters:
        benchmark (BaseLLMBenchmark): An instance of the BaseLLMBenchmark class containing the benchmarking results.
        metrics_fname (str): The filename for saving the metrics.
        metrics_path (str): The path where the metrics file will be saved.
        figures_path (str): The path where the figures will be saved.
        results (list): A list containing the benchmarking results to be saved in a CSV file.

    Returns:
        None

    """

    # Save figures
    benchmark.save_metrics_to_file(metrics_fname, metrics_path, figures_path)

    # Save per-step CSV
    results_per_step_df = pd.DataFrame(results)
    results_per_step_df.insert(0, "llm_model", benchmark.llm_model)  # Insert model type
    results_per_step_df.to_csv(
        os.path.join(metrics_path, f"{metrics_fname}.csv"), index=None
    )


def benchmark_model_step(base_dir: str, model: str, step_dir: str) -> None:
    """
    Benchmark a specific Language Model (LLM) model for a given step.

    Parameters:
        base_dir (str): The base directory where the input data is located.
        model (str): The name of the LLM model to benchmark.
        step_dir (str): The directory for a specific step within the base directory.

    Returns:
        None

    Notes:
        - Reads the expected response data, source data, and template directory for the given directories.
        - Creates output directories for metrics and figures if they do not exist.
        - Gets a benchmark instance for the specified LLM model.
        - Evaluates LLM responses for multiple templates using the benchmark instance.
        - Saves benchmarking results to files including metrics and figures.

    """
    expected_response, source_data, template_dir = read_input_data(base_dir, step_dir)

    output_dir = os.path.join(base_dir, "results", model, step_dir, "output")
    metrics_path = os.path.join(base_dir, "results", model, step_dir, "metrics")
    figures_path = os.path.join(base_dir, "results", model, step_dir, "figures")

    os.makedirs(metrics_path, exist_ok=True)
    os.makedirs(figures_path, exist_ok=True)

    benchmark = get_benchmark_instance(model)
    results = evaluate_responses(
        benchmark, expected_response, source_data, template_dir, output_dir
    )

    metrics_fname = f"metrics_{step_dir}"

    save_benchmark_results(
        benchmark, metrics_fname, metrics_path, figures_path, results
    )


if __name__ == "__main__":
    """
    Main function to benchmark responses for each model and step, and save the results.
    """
    models = ["Mixtral-8x7B", "Meta-Llama3-8B", "ChatGPT"]
    root_dir = os.path.join(os.getcwd(), "prompt_creation", "prompting")

    input_path = os.path.join(root_dir, "input")
    if os.path.exists(input_path):
        steps = [
            d
            for d in os.listdir(input_path)
            if os.path.isdir(os.path.join(input_path, d))
        ]
        for model in models:
            for step in steps:
                benchmark_model_step(root_dir, model, step)
